# Guia de ExperimentaÃ§Ã£o - YouTubeSafeKids

Este guia explica como usar os scripts de experimentaÃ§Ã£o e treinamento dos modelos BERTimbau do YouTubeSafeKids.

## ğŸ“ Estrutura dos Scripts

```
scripts/
â”œâ”€â”€ train_model.py           # Treinamento individual de modelos
â”œâ”€â”€ evaluate_model.py        # AvaliaÃ§Ã£o de modelos treinados
â”œâ”€â”€ data_preprocessing.py    # PrÃ©-processamento de dados
â”œâ”€â”€ model_comparison.py      # ComparaÃ§Ã£o entre modelos
â””â”€â”€ run_experiments.py       # OrquestraÃ§Ã£o de experimentos completos

config/
â””â”€â”€ experiment_config.json   # ConfiguraÃ§Ãµes de experimento
```

## ğŸš€ InÃ­cio RÃ¡pido

### 1. Teste RÃ¡pido

Para um teste rÃ¡pido do sistema:

```bash
python scripts/run_experiments.py --quick-test --models sentiment
```

### 2. Experimento Completo

Para executar um experimento completo:

```bash
python scripts/run_experiments.py --config config/experiment_config.json
```

## ğŸ“Š Scripts Individuais

### PrÃ©-processamento de Dados

```bash
# PrÃ©-processar dados para modelo de sentimento
python scripts/data_preprocessing.py \
    --input data/sentiment_raw.csv \
    --output data/sentiment_processed.csv \
    --model sentiment \
    --clean-text \
    --balance \
    --split

# ParÃ¢metros principais:
# --clean-text: Limpa e normaliza o texto
# --balance: Balanceia as classes
# --split: Divide em train/test
# --min-length: Comprimento mÃ­nimo do texto (padrÃ£o: 10)
# --max-length: Comprimento mÃ¡ximo do texto (padrÃ£o: 512)
```

### Treinamento de Modelos

```bash
# Treinar modelo de sentimento
python scripts/train_model.py \
    --model sentiment \
    --train-data data/sentiment_train.csv \
    --output-dir models/sentiment \
    --epochs 3 \
    --batch-size 16 \
    --learning-rate 2e-5

# Treinar modelo de toxicidade
python scripts/train_model.py \
    --model toxicity \
    --train-data data/toxicity_train.csv \
    --output-dir models/toxicity \
    --epochs 4 \
    --batch-size 16 \
    --learning-rate 3e-5 \
    --mixed-precision \
    --early-stopping
```

### AvaliaÃ§Ã£o de Modelos

```bash
# Avaliar modelo treinado
python scripts/evaluate_model.py \
    --model sentiment \
    --test-data data/sentiment_test.csv \
    --model-path models/sentiment/best_model.pt \
    --output-dir results/sentiment \
    --save-predictions

# Avaliar todos os tipos de modelo
for model in sentiment toxicity language educational; do
    python scripts/evaluate_model.py \
        --model $model \
        --test-data data/${model}_test.csv \
        --model-path models/${model}/best_model.pt \
        --output-dir results/${model}
done
```

### ComparaÃ§Ã£o de Modelos

```bash
# Comparar diferentes versÃµes de um modelo
python scripts/model_comparison.py \
    --models models/sentiment/v1.pt models/sentiment/v2.pt \
    --model-names "VersÃ£o 1" "VersÃ£o 2" \
    --test-data data/sentiment_test.csv \
    --model-type sentiment \
    --output comparison_sentiment \
    --save-predictions
```

## âš™ï¸ ConfiguraÃ§Ã£o de Experimentos

O arquivo `config/experiment_config.json` contÃ©m todas as configuraÃ§Ãµes:

### ConfiguraÃ§Ãµes de Modelo

```json
{
  "models": {
    "sentiment": {
      "model_name": "neuralmind/bert-base-portuguese-cased",
      "num_labels": 3,
      "max_length": 512,
      "learning_rate": 2e-5,
      "batch_size": 16,
      "epochs": 3,
      "dropout": 0.1
    }
  }
}
```

### ConfiguraÃ§Ãµes de Dados

```json
{
  "data": {
    "preprocessing": {
      "clean_text": true,
      "remove_duplicates": true,
      "min_length": 10,
      "max_length": 512,
      "balance_classes": true
    },
    "split": {
      "test_size": 0.2,
      "validation_size": 0.1,
      "stratify": true
    }
  }
}
```

### ConfiguraÃ§Ãµes de Treinamento

```json
{
  "training": {
    "mixed_precision": true,
    "early_stopping": {
      "enabled": true,
      "patience": 3,
      "monitor": "val_f1"
    },
    "scheduler": {
      "type": "linear",
      "warmup_ratio": 0.1
    }
  }
}
```

## ğŸ“ˆ Experimentos Automatizados

### Experimento BÃ¡sico

```bash
# Executar experimento com configuraÃ§Ã£o padrÃ£o
python scripts/run_experiments.py
```

### Experimento Personalizado

```bash
# Experimento com modelos especÃ­ficos
python scripts/run_experiments.py \
    --models sentiment toxicity \
    --data-dir /path/to/data \
    --output-dir /path/to/results

# Experimento sem treinamento (apenas avaliaÃ§Ã£o)
python scripts/run_experiments.py \
    --skip-training \
    --models sentiment

# Dry run (mostrar comandos sem executar)
python scripts/run_experiments.py \
    --dry-run \
    --verbose
```

### Teste RÃ¡pido

```bash
# Teste com configuraÃ§Ãµes reduzidas
python scripts/run_experiments.py \
    --quick-test \
    --models sentiment
```

## ğŸ“‹ Formato dos Dados

### Estrutura Esperada dos Arquivos CSV

```csv
text,label
"Este Ã© um texto positivo",positive
"Este Ã© um texto negativo",negative
"Este Ã© um texto neutro",neutral
```

### RÃ³tulos por Tipo de Modelo

- **Sentiment**: `positive`, `negative`, `neutral`
- **Toxicity**: `toxic`, `non_toxic`
- **Language**: `appropriate`, `inappropriate`
- **Educational**: `educational`, `non_educational`

## ğŸ“Š Resultados e RelatÃ³rios

### Estrutura de SaÃ­da

```
results/
â””â”€â”€ ExperimentName_20240101_120000/
    â”œâ”€â”€ models/                 # Modelos treinados
    â”‚   â”œâ”€â”€ sentiment/
    â”‚   â”œâ”€â”€ toxicity/
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ results/               # Resultados de avaliaÃ§Ã£o
    â”‚   â”œâ”€â”€ sentiment/
    â”‚   â”‚   â”œâ”€â”€ evaluation_results.json
    â”‚   â”‚   â”œâ”€â”€ predictions.csv
    â”‚   â”‚   â””â”€â”€ confusion_matrix.png
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ reports/               # RelatÃ³rios de comparaÃ§Ã£o
    â”‚   â”œâ”€â”€ sentiment_comparison_report.json
    â”‚   â”œâ”€â”€ sentiment_comparison_main_metrics.png
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ logs/                  # Logs de execuÃ§Ã£o
    â”œâ”€â”€ data/                  # Dados processados
    â”œâ”€â”€ final_report.json      # RelatÃ³rio final
    â””â”€â”€ experiment_summary.txt # Resumo do experimento
```

### MÃ©tricas DisponÃ­veis

- **Accuracy**: PrecisÃ£o geral
- **Precision**: PrecisÃ£o por classe
- **Recall**: RevocaÃ§Ã£o por classe
- **F1-Score**: MÃ©dia harmÃ´nica de precisÃ£o e revocaÃ§Ã£o
- **AUC-ROC**: Ãrea sob a curva ROC (modelos binÃ¡rios)
- **Confusion Matrix**: Matriz de confusÃ£o
- **Confidence Distribution**: DistribuiÃ§Ã£o de confianÃ§a

## ğŸ”§ SoluÃ§Ã£o de Problemas

### Problemas Comuns

1. **Erro de memÃ³ria GPU**:
   ```bash
   # Reduzir batch size
   python scripts/train_model.py --batch-size 8
   ```

2. **Dados nÃ£o encontrados**:
   ```bash
   # Verificar estrutura de diretÃ³rios
   ls -la data/
   ```

3. **Modelo nÃ£o carrega**:
   ```bash
   # Verificar caminho do modelo
   python -c "import torch; print(torch.load('models/sentiment/best_model.pt', map_location='cpu'))"
   ```

### Logs e Debugging

```bash
# Executar com logging verboso
python scripts/run_experiments.py --verbose

# Verificar logs
tail -f logs/experiment.log
```

## ğŸ¯ Exemplos de Uso

### Exemplo 1: Treinamento Completo

```bash
# 1. PrÃ©-processar dados
python scripts/data_preprocessing.py \
    --input data/sentiment_raw.csv \
    --output data/sentiment_processed.csv \
    --model sentiment \
    --clean-text --balance --split

# 2. Treinar modelo
python scripts/train_model.py \
    --model sentiment \
    --train-data data/sentiment_processed_train.csv \
    --output-dir models/sentiment_v1 \
    --epochs 5

# 3. Avaliar modelo
python scripts/evaluate_model.py \
    --model sentiment \
    --test-data data/sentiment_processed_test.csv \
    --model-path models/sentiment_v1/best_model.pt \
    --output-dir results/sentiment_v1
```

### Exemplo 2: ComparaÃ§Ã£o de Modelos

```bash
# Treinar duas versÃµes
python scripts/train_model.py \
    --model sentiment \
    --train-data data/sentiment_train.csv \
    --output-dir models/sentiment_lr2e5 \
    --learning-rate 2e-5

python scripts/train_model.py \
    --model sentiment \
    --train-data data/sentiment_train.csv \
    --output-dir models/sentiment_lr3e5 \
    --learning-rate 3e-5

# Comparar
python scripts/model_comparison.py \
    --models models/sentiment_lr2e5/best_model.pt models/sentiment_lr3e5/best_model.pt \
    --model-names "LR 2e-5" "LR 3e-5" \
    --test-data data/sentiment_test.csv \
    --model-type sentiment \
    --output comparison_lr
```

### Exemplo 3: Experimento Automatizado

```bash
# Criar configuraÃ§Ã£o personalizada
cp config/experiment_config.json config/my_experiment.json
# Editar configuraÃ§Ãµes...

# Executar experimento
python scripts/run_experiments.py \
    --config config/my_experiment.json \
    --models sentiment toxicity
```

## ğŸ“š Recursos Adicionais

- **ConfiguraÃ§Ã£o de GPU**: Ajustar `CUDA_VISIBLE_DEVICES`
- **Monitoramento**: Usar TensorBoard ou Weights & Biases
- **OtimizaÃ§Ã£o**: TÃ©cnicas de quantizaÃ§Ã£o e pruning
- **Deploy**: Scripts de conversÃ£o para produÃ§Ã£o

## ğŸ¤ ContribuiÃ§Ã£o

Para contribuir com melhorias nos scripts:

1. Teste suas modificaÃ§Ãµes
2. Documente mudanÃ§as
3. Mantenha compatibilidade
4. Adicione testes unitÃ¡rios

---

Para mais informaÃ§Ãµes, consulte a documentaÃ§Ã£o principal do projeto ou abra uma issue no repositÃ³rio.